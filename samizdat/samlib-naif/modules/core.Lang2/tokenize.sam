## Copyright 2013-2015 the Samizdat Authors (Dan Bornstein et alia).
## Licensed AS IS and WITHOUT WARRANTY under the Apache License,
## Version 2.0. Details: <http://www.apache.org/licenses/LICENSE-2.0>

##
## Samizdat Layer 2 Tokenizer
##
## This code is meant to mirror the code in the spec for tokenization as
## closely as possible, comments and all. The spec is a self-description of
## Layer 1 parsing, and the comments should be taken in light of that.


## This file is written in a subset of the language. See spec for details.
#= language core.Lang1

import core.EntityMap :: ENTITY_MAP;
import core.LangNode :: KEYWORDS, intFromDigits;
import core.Peg;  ## Used implicitly.
import core.Peg :: apply, stringFromTokenList, symbolFromTokenList;


##
## Layer 0 Rules
##
## This section consists of the definitions required to implement Layer 0,
## with comments indicating the "hooks" for higher layers.
##

## Forward declarations required for layer 2. These are all add-ons to
## layer 0 or 1 rules, used to expand the syntactic possibilities of the
## indicated base forms.
def tokInt2;
def tokMultilineComment;
def tokStringPart2;

## Parses any amount of whitespace, comments, and directives (including
## nothing at all). **Note:** The yielded result is always ignored.
def tokWhitespace = {:
    ## The lookahead here is to avoid the bulk of this rule if there's
    ## no chance we're actually looking at whitespace.
    &["# \n"]

    (
        [" \n"]+
    |
        "#" ["#!="] [! "\n"]*
    |
        ## Introduced in Layer 2.
        &"#:"  ## Avoid calling the rule unless we know it will match.
        %tokMultilineComment
    )+
:};

## Parses punctuation and operators.
##
## **Note:** This rule is expanded significantly in Layer 2.
def tokPunctuation = {:
    ## The lookahead here is done to avoid bothering with the choice
    ## expression, except when we have a definite match. The second
    ## string in the lookahead calls out the characters that are only
    ## needed in Layer 1. Yet more characters are included in Layer 2.
    &["@:.,=-+?;*/<>{}()[]" "&|!%" "\\^#"]

    (
        ## Introduced in Layer 2.
        "\\==" | "\\!=" | "\\<=" | "\\>=" | "\\<" | "\\>"
    |
        ## Introduced in Layer 2.
        "&&&" | "|||" | "^^^" | "!!!" | "<<<" | ">>>" | "..!"
    |
        ## Introduced in Layer 2.
        "==" | "!=" | "<=" | ">=" | "??" | "**" | "//" | "%%"
    |
        "->" | ":=" | "::" | ".."
    |
        ## These are introduced in Layer 1.
        "{:" | ":}"
    |
        ## Note: We check this error condition here instead of in the
        ## comment parsing code, because comments get parsed as whitespace,
        ## which gets ignored. Rather than changing to not-quite-ignore
        ## whitespace -- which would be messy -- we instead notice `#:` here
        ## where we're parsing other punctuation. This is clean but a little
        ## non-obvious, hence this comment.
        "#:" .*
        { @error{value: "Unterminated comment."} }
    |
        ## Single-character punctuation / operator.
        .
    )
:};

## Parses a single decimal digit (or spacer), returning its value.
def tokDecDigit = {:
    ["_" "0".."9"]
:};

## Parses an integer literal.
def tokInt = {:
    ## Note: Introduced in Layer 2.
    &"0"  ## Avoid calling the rule unless we know it will match.
    %tokInt2
|
    digits = tokDecDigit+
    { @int{value: intFromDigits(10, digits)} }
:};

## Parses a run of regular characters or an escape / special sequence,
## inside a quoted string.
##
## **Note:** Additional rules for string character parsing are defined
## in Layer 2.
def tokStringPart = {:
    (
        chars = [! "\\" "\"" "\n"]+
        { stringFromTokenList(chars) }
    )
|
    ## This is the rule that ignores spaces after newlines.
    "\n"
    " "*
    { "\n" }
|
    "\\"
    (
        "\\" { "\\" } |
        "\"" { "\"" } |
        "n"  { "\n" } |
        "r"  { "\r" } |
        "t"  { "\t" } |
        "0"  { "\0" }
    )
|
    ## Introduced in Layer 2.
    %tokStringPart2
:};

## Parses a quoted string.
def tokString = {:
    "\""
    parts = tokStringPart*

    (
        "\""
        { processStringParts(parts) }
    |
        { @error{value: "Unterminated string literal."} }
    )
:};

## Parses an identifier (in the usual form). This also parses keywords.
def tokIdentifier = {:
    one = ["_" "$" "a".."z" "A".."Z"]
    rest = ["_" "$" "a".."z" "A".."Z" "0".."9"]*

    {
        def name = symbolFromTokenList([one, rest*]);
        If.or { KEYWORDS.get(name) }
            { @identifier{value: name} }
    }
:};

## Parses the quoted-string identifier form.
def tokQuotedIdentifier = {:
    "\\"
    s = tokString

    { @identifier{value: Class.typeCast(Symbol, s::value)} }
:};

## "Parses" an unrecognized character. This also consumes any further
## characters on the same line, in an attempt to resynch the input.
def tokError = {:
    badCh = .
    [! "\n"]*

    {
        def msg = "Unrecognized character: ".cat(badCh.get_name());
        @error{value: msg}
    }
:};

## Parses an arbitrary token or error.
def tokToken = {:
    tokString | tokIdentifier | tokQuotedIdentifier
|
    ## This needs to be listed after the quoted identifier rule, to
    ## prevent `\"...` from being treated as a `\` token followed by
    ## a string.
    tokPunctuation
|
    ## This needs to be listed after the identifier rule, to prevent
    ## an identifier-initial `_` from triggering this rule.
    tokInt
|
    tokError
:};

## Parses a file of tokens, yielding a list of them.
def tokFile = {:
    tokens = (tokWhitespace? tokToken)*
    tokWhitespace?

    { tokens }
:};


##
## Layer 2 definitions
##
## These are all specific to parsing Layer 2 (and higher).
##

## Map from opening tokens to corresponding closers. Used when parsing
## string interpolations.
def OPEN_CLOSE_MAP = {
    @"(":  @")",
    @"[":  @"]",
    @"{":  @"}",
    @"{:": @":}"
};

## Processes a list of `stringPart` elements, yielding a literal `string`
## token, an `interpolatedString`, or an `error`.
fn processStringParts(parts) {
    ## Handle the empty string as a special case.
    If.is { Cmp.eq(parts, []) }
        { return @string{value: ""} };

    ## Coalesce adjacent literal strings in `parts`. The result is an
    ## `elems` list consisting of alternating multi-character strings and
    ## interpolation maps.
    var elems = [];
    parts.forEach { part /next ->
        ## If there is an error part (represented as an `@error` token),
        ## return it.
        If.is { Record.accepts(part) }
            {
                If.is { part.hasName(@error) }
                    { return part }
            };

        If.is { Cmp.eq(elems, []) }
            {
                ## First element. No possibility to coalesce, so special-case
                ## it.
                elems := [part];
                yield /next
            };

        If.is { String.accepts(part) }
            {
                def lastElem = elems.reverseNth(0);
                If.is { String.accepts(lastElem) }
                    {
                        ## The last item in `elems` and the current `part`
                        ## are both strings. Combine them.
                        elems := [
                            elems.sliceExclusive(0)*,
                            lastElem.cat(part)
                        ];
                        yield /next
                    }
            };

        elems := [elems*, part]
    };

    ## Check for a simple result.
    If.is { Cmp.eq(elems.get_size(), 1) }
        {
            If.value { String.accepts(elems*) }
                { elem ->
                    ## There's only one element, and it's a string. So,
                    ## we have a simple non-interpolated string. Return it.
                    return @string{value: elem}
                }
        };

    ## At this point, we have a valid interpolation.
    return @interpolatedString{value: elems}
};

## Parses a `#: ... :#` comment, with nesting.
tokMultilineComment := {:
    "#:"

    (
        %tokMultilineComment
    |
        [! ":"]
    |
        ":" !"#"
    )*

    ":#"
:};

## Parses a single binary digit (or spacer), returning its value.
def tokBinDigit = {:
    ch = ["_01"]
:};

## Parses a single hexadecimal digit (or spacer), returning its value.
def tokHexDigit = {:
    ch = ["_" "0".."9" "a".."f" "A".."F"]
:};

## Parses an integer literal.
tokInt2 := {:
    "0x"
    digits = tokHexDigit+
    { @int{value: intFromDigits(16, digits)} }
|
    "0b"
    digits = tokBinDigit+
    { @int{value: intFromDigits(2, digits)} }
:};

## Parses a single hexadecimal character. This is called during `\x...;`
## parsing.
def tokHexChar = {:
    digits = tokHexDigit+
    { Class.typeCast(String, intFromDigits(16, digits)) }
:};

## Parses a single entity-named character. This is called during `\&...;`
## parsing.
def tokNamedChar = {:
    chars = ["a".."z" "A".."Z" "0".."9" "."]+
    {
        def name = symbolFromTokenList(chars);
        ENTITY_MAP.get(name)
    }
|
    "#x"
    digits = tokHexDigit+
    { Class.typeCast(String, intFromDigits(16, digits)) }
|
    "#"
    digits = tokDecDigit+
    { Class.typeCast(String, intFromDigits(10, digits)) }
:};

## Parses the inner portion of a string interpolation expression. This
## captures a list of inner tokens, ending when parentheses and braces are
## balanced out (and ignoring other could-be-matched delimiter tokens).
def tokStringInterpolation;
tokStringInterpolation := {:
    &["({["]
    open = tokToken
    expectClose = { OPEN_CLOSE_MAP.get(open.get_name()) }

    tokens = (
        tokWhitespace?
        (
            !["(){}[]"]
            !":}"
            t = tokToken
            { [t] }
        |
            %tokStringInterpolation
        )
    )*

    tokWhitespace?
    close = tokToken

    {
        If.is { close.hasName(expectClose) }
            { [open, [].cat(tokens*)*, close] }
    }
:};

## Additional `stringPart` definitions for Layer 2.
tokStringPart2 := {:
    "\\"
    (
        "/"  { "" }
    |
        ## This is the rule that ignores an escaped newline, followed by
        ## any number of spaces.
        "\n"
        " "*
        { "" }
    |
        "x"
        (
            one = tokHexChar
            rest = ("," tokHexChar)*
            ";"
            { one.cat(rest*) }
        |
            { @error{value: "Invalid hexadecimal character literal."} }
        )
    |
        "&"
        (
            one = tokNamedChar
            rest = ("," tokNamedChar)*
            ";"
            { one.cat(rest*) }
        |
            { @error{value: "Invalid named character literal."} }
        )
    |
        format = (
            "%"
            chars = ["0".."9" "a".."z" "A".."Z" "+-."]*
            { {format: stringFromTokenList(chars)} }
        |
            { {} }
        )

        &["({["]

        (
            tokens = tokStringInterpolation
            { {format*, tokens} }
        |
            { @error{value: "Unbalanced string interpolation."} }
        )
    |
        { @error{value: "Invalid character literal escape sequence."} }
    )
:};


##
## Exported Definitions
##

## Documented in spec.
export fn tokenize(programText) {
    return apply(tokFile, programText)
};
